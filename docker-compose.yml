services:
  # Zookeeper
  zookeeper:
    image: zookeeper:3.9.0
    container_name: zookeeper
    restart: always
    hostname: zookeeper
    ports:
      - "2181:2181"
    environment:
      ZOO_MY_ID: 1
      ZOO_PORT: 2181
      ZOO_SERVERS: server.1=zookeeper:2888:3888
    volumes:
      - zookeeper_data:/data
      - zookeeper_datalog:/datalog
    networks:
      - hadoop_network
    mem_limit: 512m

  kafka:
    image: apache/kafka:3.9.0
    container_name: kafka
    restart: always
    ports:
      - "9092:9092"
    environment:
      KAFKA_CLUSTER_ID: "5L6g3nShT-eMCtK--X86sw"
      KAFKA_NODE_ID: 1
      KAFKA_PROCESS_ROLES: "broker,controller"
      KAFKA_CONTROLLER_QUORUM_VOTERS: "1@kafka:9093"
      KAFKA_LISTENERS: "PLAINTEXT://:9092,CONTROLLER://:9093"
      KAFKA_ADVERTISED_LISTENERS: "PLAINTEXT://kafka:9092"
      KAFKA_LISTENER_SECURITY_PROTOCOL_MAP: "CONTROLLER:PLAINTEXT,PLAINTEXT:PLAINTEXT"
      KAFKA_CONTROLLER_LISTENER_NAMES: "CONTROLLER"
      KAFKA_INTER_BROKER_LISTENER_NAME: "PLAINTEXT"
    volumes:
      - kafka_data:/kafka
    networks:
      - hadoop_network
    mem_limit: 1g
    healthcheck:
      test: ["CMD", "nc", "-z", "localhost", "9092"]
      interval: 10s
      timeout: 5s
      retries: 5

      

  # Hadoop NameNode
  namenode:
    image: apache/hadoop:3.3.6
    container_name: namenode
    restart: always
    hostname: namenode
    ports:
      - "9870:9870"
      - "9000:9000"
    volumes:
      - namenode_data:/hadoop/dfs/name
      - ./hadoop_conf:/opt/hadoop-3.2.1/etc/hadoop # Mount thư mục cấu hình
    networks:
      - hadoop_network
    mem_limit: 1g

  # Hadoop DataNode
  datanode:
    image: apache/hadoop:3.3.6
    container_name: datanode
    restart: always
    hostname: datanode
    volumes:
      - datanode_data:/hadoop/dfs/data
      - ./hadoop_conf:/opt/hadoop-3.2.1/etc/hadoop # Mount thư mục cấu hình
    depends_on:
      - namenode
    networks:
      - hadoop_network
    mem_limit: 1g

  # Spark Master
  spark-master:
    image: bitnami/spark:3.5.4
    container_name: spark-master
    hostname: spark-master
    restart: always
    environment:
      - SPARK_MODE=master
      - SPARK_RPC_AUTHENTICATION_ENABLED=no
      - SPARK_RPC_ENCRYPTION_ENABLED=no
      - SPARK_LOCAL_STORAGE_ENCRYPTION_ENABLED=no
      - SPARK_SSL_ENABLED=no
    ports:
      - "8080:8080"
      - "7077:7077"
    networks:
      - hadoop_network
    mem_limit: 1g

  # Spark Worker
  spark-worker-1:
    image: bitnami/spark:3.5.4
    container_name: spark-worker-1
    hostname: spark-worker-1
    restart: always
    environment:
      - SPARK_MODE=worker
      - SPARK_MASTER_URL=spark://spark-master:7077
      - SPARK_WORKER_MEMORY=1G
      - SPARK_WORKER_CORES=1
      - SPARK_RPC_AUTHENTICATION_ENABLED=no
      - SPARK_RPC_ENCRYPTION_ENABLED=no
      - SPARK_LOCAL_STORAGE_ENCRYPTION_ENABLED=no
      - SPARK_SSL_ENABLED=no
    depends_on:
      - spark-master
    networks:
      - hadoop_network
    mem_limit: 1g

  # MongoDB
  mongodb:
    image: mongo:6.0.2
    container_name: mongodb
    restart: always
    environment:
      MONGO_INITDB_ROOT_USERNAME: root
      MONGO_INITDB_ROOT_PASSWORD: password
    ports:
      - "27017:27017"
    volumes:
      - mongodb_data:/data/db
    networks:
      - hadoop_network
    mem_limit: 1g

  # Producer
  producer:
    build:
      context: ./producer
      dockerfile: Dockerfile
    container_name: producer
    restart: always
    depends_on:
      kafka:
        condition: service_healthy
      mongodb:
        condition: service_started
    environment:
      - KAFKA_BOOTSTRAP_SERVERS=kafka:9092
    networks:
      - hadoop_network
    mem_limit: 512m

  # Consumer 1
  consumer1:
    build:
      context: ./consumer1
      dockerfile: Dockerfile
    container_name: consumer1
    restart: always
    depends_on:
      kafka:
        condition: service_healthy
      mongodb:
        condition: service_started
      namenode:
        condition: service_started
      datanode:
        condition: service_started
    environment:
      - KAFKA_BROKER=kafka:9092
      - MONGO_URI=mongodb://root:password@mongodb:27017
      - HDFS_URL=http://namenode:9870
    networks:
      - hadoop_network
    mem_limit: 512m


  #Consumer 2
  consumer2:
    build:
      context: ./consumer2
      dockerfile: Dockerfile
    container_name: consumer2
    restart: always
    depends_on:
      - kafka
      - spark-master
      - spark-worker-1
      - mongodb
      - namenode
      - datanode
    environment:
      - KAFKA_BROKER=kafka:9092  
      - MONGO_URI=mongodb://root:password@mongodb:27017
      - HDFS_URL=hdfs://namenode:9000
    networks:
      - hadoop_network
    mem_limit: 512m

volumes:
  namenode_data:
  datanode_data:
  mongodb_data:
  zookeeper_data:
  zookeeper_datalog:
  kafka_data:

networks:
  hadoop_network:
    driver: bridge